{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e58e662-686d-48e0-b0c3-675945478f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Feture Enginering\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e1b7bf-a28d-48e9-8bf0-60c9d68280d7",
   "metadata": {},
   "source": [
    "## üî¢ 1. `StringIndexer`\n",
    "\n",
    "Converte valores categ√≥ricos (strings) em √≠ndices num√©ricos. √ötil para algoritmos que s√≥ trabalham com n√∫meros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c338a4c-636e-4c00-864c-ef806dee19b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|cidade|cidade_indexado|\n",
      "+------+---------------+\n",
      "|    SP|            0.0|\n",
      "|    RJ|            3.0|\n",
      "|    MG|            2.0|\n",
      "|    SP|            0.0|\n",
      "|    ES|            1.0|\n",
      "+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "dados = spark.createDataFrame([(\"SP\",), (\"RJ\",), (\"MG\",), (\"SP\",),(\"ES\",)], [\"cidade\"])\n",
    "indexador = StringIndexer(inputCol=\"cidade\", outputCol=\"cidade_indexado\")\n",
    "dados_indexados = indexador.fit(dados).transform(dados)\n",
    "dados_indexados.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7012ac7-5b18-4e61-9e73-6c754289c354",
   "metadata": {},
   "source": [
    "## üî• 2. `OneHotEncoder`\n",
    "\n",
    "Ap√≥s indexar strings com `StringIndexer`, podemos transformar os √≠ndices em vetores bin√°rios. Isso evita dar peso ou ordem falsa aos valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43186f55-afbd-4a34-8a8c-297cbb36a585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-------------+\n",
      "|cidade|cidade_indexado|    cidade_oh|\n",
      "+------+---------------+-------------+\n",
      "|    SP|            0.0|(3,[0],[1.0])|\n",
      "|    RJ|            3.0|    (3,[],[])|\n",
      "|    MG|            2.0|(3,[2],[1.0])|\n",
      "|    SP|            0.0|(3,[0],[1.0])|\n",
      "|    ES|            1.0|(3,[1],[1.0])|\n",
      "+------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCols=[\"cidade_indexado\"], outputCols=[\"cidade_oh\"])\n",
    "dados_oh = encoder.fit(dados_indexados).transform(dados_indexados)\n",
    "dados_oh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89ce4ae3-68d8-4005-85ba-e2da4eb07d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------------------------------+\n",
      "|features        |scaled_features                          |\n",
      "+----------------+-----------------------------------------+\n",
      "|[10.0,1000000.0]|[-0.7071067811865475,-0.7071067811865475]|\n",
      "|[20.0,2500000.0]|[0.7071067811865475,0.7071067811865475]  |\n",
      "+----------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "dados = spark.createDataFrame([(Vectors.dense([10.0, 1000000.0]),),\n",
    "                               (Vectors.dense([20.0, 2500000.0]),)], [\"features\"])\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True , withMean=True)\n",
    "dados_escalados = scaler.fit(dados).transform(dados)\n",
    "dados_escalados.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71de23c7-7541-4c37-817a-319911e7af25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----------------+\n",
      "|idade|salario|horas_trabalhadas|\n",
      "+-----+-------+-----------------+\n",
      "|   25|   5000|              120|\n",
      "|   40|  12000|              400|\n",
      "+-----+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dados = spark.createDataFrame([(25, 5000,120), (40, 12000,400)], [\"idade\", \"salario\", \"horas_trabalhadas\"])\n",
    "dados.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f90d089-048b-42d0-b99d-da94e4c40f9a",
   "metadata": {},
   "source": [
    "## üß± 4. `VectorAssembler`\n",
    "\n",
    "Combina m√∫ltiplas colunas em uma √∫nica coluna `features`. √â obrigat√≥rio antes de treinar qualquer modelo em MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db2231be-de1b-4ff6-8456-0f094fb5b969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----------------+--------------+\n",
      "|idade|salario|horas_trabalhadas|      features|\n",
      "+-----+-------+-----------------+--------------+\n",
      "|   25|   5000|              120| [25.0,5000.0]|\n",
      "|   40|  12000|              400|[40.0,12000.0]|\n",
      "+-----+-------+-----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"idade\",\"salario\"], outputCol=\"features\")\n",
    "dados_final = assembler.transform(dados)\n",
    "dados_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1861d363-f292-4598-a054-1d99757ddeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+--------------+-----------------------------------------+\n",
      "|idade|salario|features      |scaled_features                          |\n",
      "+-----+-------+--------------+-----------------------------------------+\n",
      "|25   |5000   |[25.0,5000.0] |[-0.7071067811865475,-0.7071067811865476]|\n",
      "|40   |12000  |[40.0,12000.0]|[0.7071067811865475,0.7071067811865476]  |\n",
      "+-----+-------+--------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True , withMean=True)\n",
    "dados_final = scaler.fit(dados_final).transform(dados_final)\n",
    "dados_final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee7a41-2764-483e-84ec-855a4f087caa",
   "metadata": {},
   "source": [
    "## üìâ 7. `PCA` (An√°lise de Componentes Principais)\n",
    "\n",
    "Reduz a dimensionalidade dos dados, mantendo a maior parte da vari√¢ncia. √ötil para eliminar colunas redundantes ou muito correlacionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e6352e2-531c-4a50-9dfd-19283a417cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = spark.createDataFrame([\n",
    "    (Vectors.dense([1.0, 2.0, 3.0]),),\n",
    "    (Vectors.dense([5.0, 6.0, 7.0]),),\n",
    "    (Vectors.dense([9.0, 10.0, 11.0]),)\n",
    "], [\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64e0e5fb-d458-4972-a14f-0a53f4eda768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------------------------+\n",
      "|features       |pcaFeatures                              |\n",
      "+---------------+-----------------------------------------+\n",
      "|[1.0,2.0,3.0]  |[-3.4641016151377544,-1.2247448713915887]|\n",
      "|[5.0,6.0,7.0]  |[-10.392304845413262,-1.2247448713915894]|\n",
      "|[9.0,10.0,11.0]|[-17.32050807568877,-1.2247448713915896] |\n",
      "+---------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "modelo_pca = pca.fit(dados)\n",
    "dados_pca = modelo_pca.transform(dados)\n",
    "dados_pca.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e9cb959-2806-475a-8d7a-732e4304e15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vari√¢ncia explicada por cada componente: [1.00000000e+00 8.52687525e-18]\n",
      "Vari√¢ncia total explicada pelos 2 componentes: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Vari√¢ncia explicada por cada componente:\", modelo_pca.explainedVariance.toArray())\n",
    "print(\"Vari√¢ncia total explicada pelos\", pca.getK(), \"componentes:\", sum(modelo_pca.explainedVariance))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
